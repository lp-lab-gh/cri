Szkolenie Kubernetes - Agenda
—————————————————————————————————————————

Podman
Przegląd zagadnień związanych z Podman
Podman to narzędzie do zarządzania kontenerami, które jest alternatywą dla Dockera. Główne różnice między Podmanem a Dockerem to:
Bezpieczeństwo: Podman nie wymaga działania usługi daemon i może być używany bez uprawnień administratora (rootless containers).
Kompatybilność: Obsługuje obrazy i polecenia zgodne z Dockerem.
Podman vs Docker:
Docker działa w modelu klient-serwer, a Podman działa jako samodzielne narzędzie.
Podman pozwala na uruchamianie kontenerów bez potrzeby działania centralnego daemona.
Tworzenie obrazów przy użyciu narzędzia Podman
Podobnie jak Docker, Podman pozwala na budowanie obrazów kontenerów z użyciem Dockerfile. Proces obejmuje:
Definiowanie warstw obrazu w Dockerfile.
Użycie narzędzia podman build do stworzenia obrazu.
Weryfikację stworzonego obrazu za pomocą podman images.
Tworzenie obrazów przy użyciu podman build
Składnia polecenia: podman build -t nazwa-obrazu .
Opcje dodatkowe:
--file – określenie alternatywnego pliku Dockerfile.
--layers – wykorzystanie istniejących warstw w celu przyspieszenia procesu budowy.
Zarządzanie obrazami: push, pull i tagowanie
Pobieranie obrazów: podman pull obraz
Wysyłanie obrazów do rejestru: podman push obraz registry.example.com/użytkownik/obraz
Tagowanie obrazów: podman tag obraz nowa-nazwa
Praktyczne zastosowanie
Podman może być stosowany w różnych scenariuszach, takich jak:
Uruchamianie aplikacji w izolowanych kontenerach.
Tworzenie lekkich środowisk testowych.
Automatyzacja procesów CI/CD.
Użycie w systemach z podwyższonymi wymaganiami bezpieczeństwa.
Uruchamianie kontenerów za pomocą Podman
Uruchomienie prostego kontenera: podman run -d --name kontener-example obraz
Uruchomienie kontenera z interaktywnym dostępem: podman run -it --rm obraz bash
Uruchomienie kontenera z mapowaniem portów: podman run -d -p 8080:80 --name kontener-web serwer-www
Uruchomienie kontenera z woluminem do przechowywania danych: podman run -d -v /ścieżka-lokalna:/ścieżka-kontenerowa --name kontener-z-danymi obraz
Uruchomienie kontenera dla konkretnego adresu IP: podman run -d --ip 192.168.1.100 -p 8080:80 --name kontener-ip obraz
Wyświetlenie działających kontenerów: podman ps
Wyświetlanie wszystkich kontenerów (w tym zatrzymanych): podman ps -a
Zatrzymanie kontenera: podman stop kontener-example
Usunięcie kontenera: podman rm kontener-example
Przekształcenie workflow Docker do Podman
Większość poleceń Dockera ma swoje odpowiedniki w Podmanie.
Można łatwo zastąpić docker przez podman w skryptach.
Podman posiada aliasy umożliwiające użytkownikom przyzwyczajonym do Dockera łatwe przejście: alias docker=podman
Narzędzie podman system migrate pomaga w migracji istniejących konfiguracji Dockera do Podmana.

Instalacja klastra Kubernetes:
W celu instalacji klastra Kubernetes wykorzystamy narzędzie kubeadm.
Minimalne wymagania dotyczące node’a klastra to co najmniej dwa node’y o następujących parametrach:
OS: Ubuntu
RAM: 2GB lub więcej
CPU: 2 CPU lub więcej dla node’a control-plane
Połączenie sieciowe pomiędzy nodami
Przed zainstalowanie klastra niezbędne jest przygotowania runtime kontenerów oraz narzędzie kubetools: kubeadm, kubeinit, kube-proxy oraz kubectl.
Runtime dla kontenerów jest niezbędny aby móc korzystać z kontenerów
Kubernetes wspiera różnorodne środowiska runtime dla kontenerów:
containerd
CRI-O
Docker Engine
Mirantis Container Runtime
Poniżej znajduje się link do pobrania repozytorium, z którego pobrać można skrypt instalacyjny środowiska runtime na Wasze stacje robocze:

https://github.com/lp-lab-gh/cri.git



setup-container.sh



Przed instalacją klastra skorzystamy ze skryptu instalacyjnego dla narzędzi Kubernetes, który zawiera
kubeadm: wykorzystamy go do instalacji i zarządzania naszym klastrem Kubernetes
kubelet: core service, który uruchamia pody
kubectl: klient/interfejs, którym będziemy wykonywać polecenia do uruchamiania i zarządzania aplikacjami Kubernetetes
https://github.com/lp-lab-gh/cri.git


setup-kubetools.sh




- Przed skonfigurowaniem klastra warto więc dowiedzieć się nieco o wymaganiach sieciowych dla różnych węzłów. 
W Kubernetes wykorzystywane są różne rodzaje komunikacji sieciowej.

Pierwszym z nich jest komunikacja między węzłami.

Jest to część obsługiwana przez sieć fizyczną, w której wszystkie węzły są ze sobą bezpośrednio połączone.

Istnieje komunikacja External-to-Service, która jest obsługiwana przez zasoby Kubernetes Service.

Istnieje komunikacja Pod-to-Service, która również jest obsługiwana przez usługi Kubernetes. 

Komunikacja Pod-to-Pod jest obsługiwana przez wtyczkę sieciową.

I wreszcie, istnieje komunikacja kontener-kontener, która jest stosowana tylko wtedy, gdy wiele kontenerów działa w tym samym Pod. 

Komunikacja między tymi kontenerami jest obsługiwana w ramach samego Pod. 

Ważną częścią konfiguracji klastra Kubernetes jest dodatek sieciowy, ponieważ bez dodatku sieciowego nie można zdefiniować sieci Pod. 

Obecnie dodatek sieciowy jest dostarczany przez ekosystem i dostępne są różne dodatki sieciowe. 

Problematyczne jest to, że sam Vanilla Kubernetes nie jest dostarczany z domyślnym dodatkiem, a to dlatego, że Cloud Native Computing Foundation nie chce faworyzować jednego konkretnego rozwiązania

Oznacza to, że po uruchomieniu kubeadm należy samodzielnie skonfigurować dodatek sieciowy. 

Kubernetes udostępnia Container Network Interface, czyli CNI, jako ogólny interfejs, który umożliwia łatwe korzystanie z różnych wtyczek. 

Dostępność konkretnych funkcji zależy od używanej wtyczki sieciowej. 

Należy na przykład szukać obsługi Networkpolicy, IPv6 lub Role Based Access Control. 

Dostępnych jest wiele dodatków sieciowych. Chcę tylko wspomnieć o czterech najważniejszych.

Cilium jest tym, którego będziemy używać w tym szkoleniu.

Najpopularniejszym rozwiązaniem jest Calico.
 
Istnieje Flannel, który jest ogólnym dodatkiem sieciowym, który był często używany w przeszłości, ale nie obsługuje polityki sieciowej. 

Istnieje Multus, który jest wtyczką, która może współpracować z wieloma wtyczkami sieciowymi i jest obecnie domyślną wtyczką w OpenShift, która jest bardzo ważną dystrybucją Kubernetes. 

Jest też Weave, wspólny dodatek sieciowy, który obsługuje również typowe funkcje.



W dalszej części tej szkolenia pokażę, jak zainstalować klaster przy użyciu kubeadm. 

Zanim to zrobimy, dobrze jest wiedzieć trochę o tym, co się wydarzy.

Tak więc, podczas uruchamiania kubeadm init, wykonywane są różne fazy. 

Zaczyna się od preflight. 

Zapewnia to spełnienie wszystkich warunków i pobranie podstawowych obrazów kontenerów. 

Następnie zajmuje się certyfikatami, generowany jest samopodpisany Kubernetes Certificate Authority i tworzone są powiązane certyfikaty dla apiserver, etcd i proxy. 

Kubernetes często korzysta z tych certyfikatów PKI i są one bardzo ważne. Następnie znajduje się kubeconfig, w którym konfigurowane są pliki konfiguracyjne dla podstawowych usług Kubernetes. 

Następnie można uruchomić kubelet. 

Gdy kubelet jest już dostępny, konfigurowany jest control-plane. 

W tym celu kubelet używa statycznych manifestów pod. Tworzy je i uruchamia dla apiservera, menedżera kontrolera i schedulera. 

W tym momencie używany jest etcd. Tworzone i uruchamiane są więc statyczne manifesty Pod dla etcd. 

Następnie procedura jest kontynuowana wraz z przesyłaniem konfiguracji. ConfigMaps są tworzone dla ClusterConfiguration i konfiguracji komponentów kubelet. 

Upload-certs jest miejscem, w którym wszystkie certyfikaty są przesyłane do /etc/kubernetes/pki na węźle control-plane. 

Następnie węzeł jest oznaczany jako control plane i generowany jest token bootstrap.

Jest to token, którego można użyć do dołączenia do innych węzłów. 

Następnym krokiem jest kubelet-finalized, który finalizuje ustawienia kubeleta. 

Gdy to zrobimy, pozostaje już tylko jeden krok do wykonania, a jest nim dodatek. W tej części instalowane są dodatki coredns i kube-proxy. 

Po wykonaniu wszystkich tych czynności mamy już działające środowisko klastra Kubernetes. 

Teraz ważne jest, abyście wiedzieli trochę o tych różnych fazach, ponieważ w przypadku, gdy coś pójdzie nie tak, warto wiedzieć, gdzie coś poszło źle i jak wpłynie to na wszystko inne, co może być przydatne do rozwiązywania problemów.


Instalacja klastra - zestaw skryptów:

Sklonowanie repozytorium ze skryptami:

git clone https://github.com/lp-lab-gh/cri,git

Instalacja CRI: 

sudo ~/cri/setup-container.sh

Instalacja kubetools:

sudo ~/cri/setup-kubetools.sh

Modyfikacja parametrów w kubeadm-config.yaml:

vim ~/cri/kubeadm-config.yaml


apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: stable
apiServer:
  certSANs:
  - "k8s-X-master-1.k8s.lp-lab.cloud"
controlPlaneEndpoint: "k8s-X-master-1.k8s.lp-lab.cloud:6443"


Instalacja klastra:

sudo kubeadm init --config=~/cri/kubeadm-config.yaml

Przygotowanie klienta “kubectl”:

	- mkdir ~/.kube
	- sudo cp -i /etc/kubernetes/admin.conf ~/.kube/config
	- sudo chown $(id -u):$(id -g) .kube/config
Instalacja wtyczki sieciowej Cilium:
sudo snap install helm --classic
kubectl -n kube-system delete ds kube-proxy
kubectl -n kube-system delete cm kube-proxy
helm repo add cilium https://helm.cilium.io/
helm upgrade --install cilium cilium/cilium --version 1.16.4 --namespace kube-system --set kubeProxyReplacement=true --set k8sServiceHost=k8s-X-master-1.k8s.lp-lab.cloud --set k8sServicePort=6443 --set hubble.relay.enabled=true --set hubble.ui.enabled=true
Operacje z kubeadm init:
--apiserver-advertise-address
--config
--dry-run
--pod-network-cidr
--service-cidr
Dodawanie nodeów do klastra:
sudo kubeadm join
W przypadku utraty join token’u lub jego wygaśnięcia używamy polecenia:
sudo kubeadm token create --print-join-command
Na nodach należy uruchomić polecenie:
sudo kubeadm join k8s-X-master-1.k8s.lp-lab.cloud:6443 --token YYY -- discovery-token-ca-cert-hash sha256:ZZZ 
Zarządzanie kontekstami:
kubectl get config
kubectl set-context
kubectl use-context

Używanie deployment’ów:
Deployment to podstawowa opcja uruchamiania kontenerów na Kubernetes
Deployment jest odpowiedzialny za uruchamianie i skalowanie podów
Deployment używa ReplicaSet aby zarządzać skalowalnością
Deployment oferuje politykę wdrażania RollingUpdate, aby uzyskać bezprzerwową funkcjonalność aktualizacji aplikacji.
Uruchomienie deploymentu za pomocą tzw. podejścia imperatywnego:
kubectl create deployment -h
kubectl create deployment test-nginx --image nginx --replicas=3
kubectl get po -w
kubectl get all
Używanie DaemonSet:
DaemonSets są powszechnie używane do uruchamiania agentów.
Jest to zasób, który uruchamia jedną instancję aplikacji na każdym węźle klastra. Z tego powodu nie ma liczby replik zdefiniowanych w DaemonSet. Po prostu uruchamia jednego agenta na każdym węźle klastra.
Sprawdźmy czy mamy jakieś obiekty tego typu w klastrze:
kubectl get ds -A
Powoływanie obiektu DaemonSet dla obrazu nginx:
kubectl create deploy mydaemon --image=nginx --dry-run=client -o yaml > mydaemon.yaml
Podnosimy do edycji:
vi mydaemon.yaml
zmieniamy:
Deployment > DaemonSet
 i usuwamy:
- replicas:1
- strategy: {}
kubectl apply -f mydaemon.yaml
kubectl get ds -n default
Używanie StatefulSet:
StatefulSet oferuje kilka funkcji, które są potrzebne w aplikacjach stanowych.
- Zapewnia gwarancje dotyczące kolejności i unikalności podów
- Utrzymuje stały identyfikator dla każdego z podów, które tworzy, a pody w StatefulSet nie są wymienne.
- Każdy Pod ma trwały identyfikator, który utrzymuje podczas zmiany harmonogramu.
- Unikalne identyfikatory podów ułatwiają dopasowanie istniejących woluminów do zastąpionych podów.
Spójrzmy na przykładowy plik statefuldemo.yaml:
cat statefuldemo.yaml
kubectl apply -f statefuldemo.yaml
kubectl get statefulset lub kubectl get sts
Rozwiązanie problemu ze storage
Uruchamianie pojedynczych podów:
Główne ryzyka:
Brak ochrony zrównoważonego obciążenia
Brak LoadBalancing’u
Brak bezprzerwowej aktualizacji aplikacji
Stosujemy pojedyncze pody wyłącznie w celu testowania, rozwiązywania problemów lub analizy aplikacji/klastra.
W każdym innym razie stosujemy obiekty tj. Deployment, DaemonSet lub Statefulset
Uruchomienie przykładowego pojedynczego pod’a:
kubectl run pod -h | less
kubectl run pod-with-sleep --image=busybox -- sleep 3600
kubectl get po
Zarządzanie inicjalizacją podów:
Jeśli więc wymagane jest przygotowanie przed uruchomieniem głównego kontenera, odpowiedzią jest init container. Init container’y są częścią pod’a i są uruchamiane razem z głównym kontenerem. Init container działa do końca, a po jego zakończeniu można uruchomić główny kontener. Init container’y są używane w każdym przypadku, w którym wymagana jest wstępna konfiguracja aplikacji.
Przykładowy plik:
kubectl apply -f initme.yaml
Modyfikacja parametrów na sleep 30

apiVersion: v1
kind: Pod
metadata:
  name: init-demo-sleep
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  initContainers:
  - name: install
    image: busybox
    command:
    - “sleep”
    - “30”



kubectl apply -f initmesleep.yaml
Skalowanie aplikacji:
Używamy polecenia kubectl scale w stosunku do obiektów tj. Deployment, ReplicaSet lub StatefulSet:
kubectl scale deployment myapp  --replicas=3
Użycie komendy kubectl get all z parametrem  --selector
kubectl get all --selector app=test-nginx
Alternatywą jest użycie obiektu: HorizontalPodAutoscaler.
Używanie Sidecar Containers:
W niektórych przypadkach potrzebny jest jednak dodatkowy kontener do modyfikacji lub prezentacji danych wygenerowanych przez główny kontener. Zdefiniowano też pewne szczególne przypadki użycia, którymi są:
 sidecar, który zapewnia dodatkową funkcjonalność głównemu kontenerowi 
ambassador, który jest używany jako proxy do zewnętrznego łączenia kontenerów
adapter, który służy do standaryzacji lub normalizacji danych wyjściowych głównego kontenera.
Przykład w oparciu o plik sidecarlog.yaml 
kubectl apply -f sidecarlog.yaml
kubectl exec -it two-containers -c nginx-container -- cat /usr/share/nginx/html/index.html
Zarządzanie storage:
Trwałe woluminy są specyficzne dla każdego z namespace’ów, więc to administrator namespace’a musi upewnić się, że trwałe woluminy są dostępne.
W jaki sposób zamierzasz połączyć się z poda do trwałego woluminu? Zamierzamy to zrobić za pomocą PVC, co jest skrótem od persistent volume claim.To żądanie trwałego wolumenu jest niezależnym zasobem API, który zostanie skonfigurowany jako wolumen pod’a. Wystarczy więc ustawić typ woluminu pod na persistence volume claim. 
Żądanie trwałego wolumenu to po prostu żądanie pamięci masowej. Żądanie to jest wyrażone jako rozmiar i to, czy pamięć masowa powinna być zapisywalna (tzw. pamięć RWX) czy tylko do odczytu (tzw. pamięć RWO). Jak to działa? Działa to w ten sposób, że uruchamiasz swój pod i PVC w określonym namespace, a następnie PVC sprawdzi w tym namespace, czy dostępna jest określona pamięć masowa. Jeśli istnieje pasujący trwały wolumen, to jest to proste, a PVC zostanie powiązane z pasującym PV. Istnieje jednak opcja elastyczności, która jest realizowana za pomocą StorageClass. StorageClass jest używany do automatycznego dostarczania pamięci masowej, jeśli nie jest ona dostępna w momencie nadejścia żądania PVC. Aby móc pracować z obiektami StorageClass, potrzebny jest również odpowiedni provider dla trwałego woluminu (PV).
Czym jest PV Provisione? Dostawca pamięci masowej komunikuje się z pamięcią masową dostępną w określonym namespace. 
StorageClass to obiekt API reprezentujący Provisioner’a pamkięci. Tak więc bez względu na to, jaką masz pamięć masową, tak długo, jak masz provisioner’a i StorageClass która z nim współpracuje, StorageClass może przekazywać pamięć masową na żądanie PVC. 
Tak więc w przypadku, gdy PVC zgłasza żądanie pamięci masowej i nie ma dostępnej pamięci masowej, klasa pamięci masowej przechwyci to żądanie i utworzy trwały wolumin (PV) na żądanie zgodnie z wymaganiami określonymi w PVC.Dzięki temu Kubernetes może dynamicznie przydzielać przestrzeń dyskową, gdy jest ona potrzebna.
Konfiguracja PersistenceVolumes:
Przykład:
vi pv.yaml
kubectl apply -f pv.yaml
Konfiguracja PersistenceVolumeClaim:
Przykład:
vi pvc.yaml
kubectl apply -f pvc.yaml
Troubleshooting
Konfiguracja StorageClass:
Instalacja serwera NFS na node’dzie - “k8s-X-master-1”:
na nodzie control-plane: sudo apt install nfs-server -y
sudo mkdir /nfsexport
sudo sh -c ‘echo “/nfsexport *(rw,no_root_squash)” > /etc/exports’
sudo systemctl restart nfs-server
Instalacja klienta NFS na node’ach - “k8s-X-worker-1” i “k8s-X-worker-2”:
sudo apt install nfs-client
showmount -e k8s-X-master-1
Instalacja dodatku nfs-subdir-external-provisioner na węźle “k8s-X-master-1”:
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner
helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=192.168.0.X --set nfs.path=/nfsexport
Zarządzanie PVC:
kubectl get pv
kubectl apply -f nfs-provisioner-pvc-test.yaml
kubectl get pv,pvc
Ćwiczenia z PVC i SC:
kubectl apply -f another-pvc-test.ymal
kubectl get pvc
kubectl patch storageclass nfs-client -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
kubectl get pvc
Konfiguracja ConfigMap i Secret’ów
ConfigMap jest zasobem API używanym do przechowywania danych specyficznych dla witryny. Secret to ConfigMap zakodowany w base64. 
Nie ma żadnej fundamentalnej różnicy między Secret i ConfigMap, z jedynym wyjątkiem, że Secret są trudniejsze do odczytania, ponieważ najpierw trzeba je zdekodować. ConfigMaps są używane do przechowywania zmiennych środowiskowych, parametrów startowych lub plików konfiguracyjnych. Gdy plik konfiguracyjny jest używany w ConfigMap lub Secret, jest montowany jako wolumin, aby zapewnić dostęp do jego zawartości.
Przykład w oparciu o webserver.yaml z repozytorium.
Zarządzanie dostęp do aplikacji (CNI, Services):
Omówienie CNI:
Calico
Flannel
Cilium
Omówienie usług (services):
NodePort
ClusterIP
LoadBalancer
Konfiguracja Ingress:
Instalacja:
helm upgrade --install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --namespace ingress-nginx --create-namespace
kubecti get pods -n ingress-nginx
kubectl create deploy nginxsvc --image=nginx --port=80
kubecti expose deploy nginxsvc
Wsparcie dla ruchu HTTP i HTTPS
kubectl create ingress nginxsvc --class=nginx --rule=nginxsvc.info/*=nginxsvc:80
kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80
echo "127.0.0.1 nginxsvc.info" >> /etc/hosts
curl nginxsvc.info:8080
Konfiguracja i omówienie IngressClass
kubectl get ingressclass -o yaml
Przykłady:
Przykład z webshop-apps:
kubectl get deployment kubectl get svc webshop
kubectl create ingress webshop-ingress --rule="/=webshop:80" --rule="/hello=newdep:8080"
sudo vim /etc/hosts
127.0.0.1 webshop.info
kubectl get ingress
kubectl describe ingress webshop-ingress
Ingress Rule dla różnych ścieżek:
kubectl create ingress mygress --rule="/mygress=mygress:80" --rule="/yourgress=yourgress:80"
Różne virtual host’y:
kubectl create ingress nginxsvc--class=nginx --rule=nginxsvc.info/*=nginxsvc:80 --rule=otherserver.org/*=otherserver:80
Używanie port forwarding’u do bezpośredniego dostępu do aplikacji
Zarządzanie klastrem:
Analiza node’ów klastra
Używanie crictl do zarządzania i analizowania kontenerów na węźle
Uruchomienie tzw. static pod
ssh k8svc@k8s-X-worker1
sudo vi /etc/kubernetes/manifests/staticpod.yaml
Zarządzanie stanem node’ów
Drain
kubectl drain --ignore-daemonsets --delete-emptydir-data
Cordon
kubectl cordon <node>
kubectl describe node <node>
kubectl get nodes
Uncordon
kubectl uncordon <node>
Zarządzanie usługami node’a
ps aux | grep kubelet
ps aux | grep containerd
systemctl status kubelet
sudo systemctl stop kubelet
sudo systemctl start kubelet
Przeprowadzanie zadań konserwacyjnych na node’ach
Instalacja i przegląd funkcji metrics-server
dokumentacja: https://github.com/kubernetes-sigs/metrics-server.git
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl -n kube-system get pods # look for metrics-server
kubectl -n kube-system edit deployment metrics-server
W spec.template.spec.containers.args, zmień na następujące wartości:
--kubelet-insecure-tls
--kubelet-preferred-address-types=InternallP,ExternalIP,Hostname
kubectl -n kube-system logs metrics-server<TAB> powinna pokazać "Generating self-signed cert" i "Serving securely on [::]443
kubectl top pods --all-namespaces zwróci najbardziej aktywne pody
Backup bazy danych etcd
sudo apt install etcd-client
sudo etcdctl --help; sudo ETCDCTL_API=3 etcdctl --help
ps aux | grep etcd
sudo ETCDCTL_API=3 etcdctl –endpoints=localhost:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get / --prefix --keys-only
sudo ETCDCTL_API=3 etcdctl –endpoints=localhost:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot save /tmp/etcdbackup.db
Przywracanie kopii bazy danych etcd
sudo ETCDCTL_API=3 etcdctl snapshot restore /tmp/etcdbackup.db data-dir /var/lib/etcd-backup
Usługi core’owe Kubernetes podczas operacji restore’u etcd muszą zostać zatrzymane
Aby to wykonać należy przesunąć tymczasowo zawartość katalogu /etc/kubernetes/manifests/*.yaml to /etc/kubernetes/
Proces kubelet co jakiś czas sprawdza zawartość katalogu
Poleceniem sudo crictl ps zweryfikować można czy usługi się zatrzymały
Przeprowadzanie aktualizacji węzłów
Klaster o wysokiej dostępności - opis
Klaster o wysokiej dostępności - wdrożenie
Zarządzanie scheduler’em
Kube-scheduler bierze na siebie zadania związane z znalezieniem node’a w celu zaschedulowania nowych podów
Node’y mogą być filtrowane wg specyficznych wymagań tj.:
Wymagania co do zasobów sprzętowych
Reguł affinity i anti-affinity
Taints i Tolerations
Scheduler na podstawie wprowadzonych danych wybiera node’y nadając im punktację a następnie wybiera najwyżej ocenionego node’a w celu zaschedulowania pod’a zgodnego z wymaganiami
Kubelet zajmuje się pobranie obrazu i instalacją pod’a na wskazanych przez kube-scheduler’a node’dzie
Ustawianie preferencji node’a
kubectl label node
nodeSelector
nodeName
Ustawienie affinity i anti-affinity
pod-with-node-affinity.yaml
pod-with-node-anti-affinity.yaml
pod-with-pod-affinity.yaml
Zarządzanie taints i tolerations
Przykłady:
taint-tolerations.yaml
taint-tolerations2.yaml
Omówienie LimitRange i Quota.
Konfiguracja LimitRange:
Przykład:
limitrange.yaml
Sieć w Kubernetes:
Omówienie sieci CNI i plugin’ów sieciowych
Omówienie service auto registration
Przykłady:
Polityki networkPolicy do zarządzania ruchem pomiędzy podami
Konfiguracja networkPolicy w klastrze
Zarządzanie bezpieczeństwem klastra
Omówienie dostępu do API klastra
Zarządzanie security context
Używanie kont serwisowych do dostępu do API klastra
Tworzenie polity RBAC
Konfiguracja ról klastra oraz RoleBindings
Tworzenie konto użytkowników Kubernetes
Logowanie, Monitorowanie oraz rozwiązywanie problemów z klastrem
Monitorowanie zasobów Kubernetes
kubectl top pod
kubectl top nodes
Zapoznanie się z narzędziami pomocnymi w rozwiązywaniu problemów z klastrem
kubectl describe
kubectl events
Rozwiązywanie problemów z aplikacjami
kubectl get
kubectl logs
Rozwiązywanie problemów z węzłami klastra Kubernetes:
sudo systemctl status kubelet
sudo systemctl restart kubelet
sudo openssl x509 -n /var/lib/kubelet/pki/kubelet.crt -text
kubectl get pods -n kube-system
Rozwiązywanie problemów z dostępem do aplikacji pracujących w ramach klastra:
kubectl get endpoints
Harbor jako repozytorium obrazów
Dlaczego warto używać Harbor do bezpiecznego przechowywania obrazów?
Instalacja Harbor
Konfiguracja i przesyłanie obrazów do Harbor
Monitorowanie i diagnostyka
 Podstawy monitorowania
Prometheus, Grafana i Kubernetes Dashboard
Instalacja i przegląd zdarzeń
Integracja wdrażania z GitOps
Przegląd strategii GitOps
Zasady i korzyści GitOps dla CI/CD
Repozytoria
Struktura repozytoriów gotowych na GitOps
Konfigurowanie przepływu pracy GitOps
Praktyczny przykład z wykorzystaniem ArgoCD


